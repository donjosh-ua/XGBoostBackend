from __future__ import division
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
'''
Definicion de las funciones de perdida (Loss).
'''


def SSE(true, predicted):
    #Error cuadr√°tico medio 
    #print(true.dtype) #Float 32 
    #print(true.shape)
    #print(predicted)
    #print(predicted.dtype) # float 32 requiere grad
    Loss = nn.MSELoss()
                
    return (Loss(predicted, true))

def cross_entropy(true, predicted):
    #print(true)   #Entropia cruzada - ojo va con la softmax como activacion final 
    #print(true.dtype)
    #print(true.shape)
    #print(predicted)
    #print(predicted.dtype)
    #print(predicted.shape)
    #predicted = torch.LongTensor(predicted)
    #return (F.nll_loss(F.log_softmax(true, 1), predicted))
    Loss = nn.CrossEntropyLoss()
    return (Loss(predicted, true))

def NLLLoss(true, predicted):
    #print(true)  # Logaritmico negativo - ojo va con la logsoftmax como activacion final 
    #print(true.dtype) # LongTensor
    #print(true.shape)
    #print(predicted)
    #print(predicted.dtype) # FloatTensor 32
    #print(predicted.shape)
    #predicted = torch.LongTensor(predicted)
    Loss = nn.NLLLoss()
    return (Loss(predicted, true))

def BCELoss(true, predicted):
    #print(true)  # Cross entropy binario - ojo va con la sigmoide como activacion final - solo se utiliza si quiero una etiqueta por cada 
                   #valor 
    true = np.float32(true)
    true =torch.tensor(true) # Porque la entrada debe ser flotante y no int como lo es true 
    #print(predicted)
    Loss = nn.BCELoss()
    return (Loss(predicted, true))