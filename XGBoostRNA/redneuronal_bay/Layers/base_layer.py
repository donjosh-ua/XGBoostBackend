from __future__ import division
from abc import abstractmethod, ABCMeta
import numpy as np
from redneuronal_bay.funcion_activacion import *
from redneuronal_bay import RedNeuBay
import torch
import math
from torch import FloatTensor
from torch.autograd import Variable
from torch.nn.parameter import Parameter


class BaseLayer:
    """
    Creo un objeto para las capas de la red
    """

    __metaclass__ = ABCMeta

    def __init__(self, input_dim, output_dim, funcion_activacion):
        """
        :input_dim: Input Dimension for this Layer
        :output_dim: Output Dimension for this Layer
        :transformFunction: Non-Linear transform puede ser aplicado en la salida ojo revisar si esta bien ???? o debo incluir como
        :                   entada
        """

        assert (
            input_dim > 0
            and isinstance(input_dim, int)
            and output_dim > 0
            and isinstance(output_dim, int)
        ), (
            "Las dimensiones de la entrrada y salida deben ser mayores a cero, "
            "Valores input_dim = {0} y output_dim = {1}".format(input_dim, output_dim)
        )

        torch.manual_seed(1234)
        #######torch.manual_seed(2468)

        self.input_dim = input_dim
        self.output_dim = output_dim
        self.funcion_activacion = funcion_activacion
        self.weights = Parameter(torch.Tensor(output_dim, input_dim))  # adicionado
        self.bias = Parameter(torch.Tensor(output_dim))  # adicionado

        # self.weights = Variable(torch.FloatTensor(self.__set_weights(self.input_dim, self.output_dim))) #origina

        self.weights = torch.nn.init.kaiming_uniform_(
            torch.Tensor(output_dim, input_dim), a=math.sqrt(5)
        )  # adicionado
        # self.weights = torch.nn.init.kaiming_normal_(torch.Tensor(output_dim, input_dim), a=0, mode='fan_out') #adicionado

        # -------------------------------------------------------------------------------
        # Si coloco mode= fan_in conserva la magnitud de la varianza de los pesos en el pase hacia adelante . La elección 'fan_out'conserva
        # las magnitudes en el pase hacia atrás. Podemos escribir lo siguiente.
        # ----------------------------------------------------------------------------------------------

        self.weights = self.weights.T  # adicionado
        fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(
            self.weights
        )  # adicionado
        limite = 1 / math.sqrt(fan_in)  # adicionado
        self.bias = torch.nn.init.uniform_(self.bias, -limite, limite)  # adicionado
        self.bias = self.bias.T  # adicionado
        # self.weights = torch.nn.init.uniform_(torch.Tensor(output_dim, input_dim)) # adicionado

        # self.bias = Variable(torch.FloatTensor(np.ones((1,self.output_dim))))  # original

    # def __set_weights(self,input_dim, output_dim):
    #    np.random.seed(1234)
    #    return np.random.normal(0,1,output_dim*input_dim).reshape(input_dim,output_dim)

    @abstractmethod
    def derivative(self, x):
        pass

    def __str__(self):
        """
        Me anula la funcion de impresion de python.
        :regresa: capa detallada.
        """
        return (
            "{:>10s}{:>18d}\n".format("Input dimension", self.input_dim)
            + "{:>10s}{:>17d}\n".format("Output dimension", self.output_dim)
            + "{:>s}{:>18s}\n".format(
                "Activation Function", self.funcion_activacion.__name__
            )
            + "{:>10s}{:>23s}\n".format("Weights Shape", self.weights.shape)
        )
