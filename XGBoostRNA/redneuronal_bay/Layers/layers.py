from redneuronal_bay.Layers.base_layer import BaseLayer
from redneuronal_bay.funcion_activacion import *
import numpy as np
import torch
from torch import FloatTensor
from torch.autograd import Variable

class Sigmoid_Layer(BaseLayer):

    def __init__(self,input_dim,output_dim):
        '''
        :input_dim : Dimension de entrada de la capa
        :output_dim: dimension de salida de la capa
        '''
        BaseLayer.__init__(self,input_dim,output_dim, funcion_activacion = sigmoide)

    def derivative(self, x):
        '''
        :x: Entrada de la funcion sigmoide.
        :regresa: Derivada de la sigmoide
        '''
        x = x.detach().numpy()
        dv = np.multiply((1.0/(1.0 + np.exp(-x))), (1 - (1.0/(1.0 + np.exp(-x)))))
        return (Variable(torch.FloatTensor(dv),requires_grad=True))


class Tanh_Layer(BaseLayer):

    def __init__(self,input_dim,output_dim):
        '''
        :input_dim : Dimension de entrada de la capa
        :output_dim: dimension de salida de la capa
        '''
        BaseLayer.__init__(self,input_dim,output_dim, funcion_activacion = tan_h)

    def derivative(self, x):
        '''
        :x: Entrada de la funcion sigmoide.
        :regresa: Derivada de la tanh
        '''
        #return (1 - np.square(x))
        return (1 - torch.square(x))
        #return (x)
    

class ReLU_Layer(BaseLayer):

    def __init__(self,input_dim,output_dim):
        '''
        :param input_dim: input dimension of layer
        :param output_dim: output dimension of layer
        '''
        BaseLayer.__init__(self, input_dim, output_dim, funcion_activacion=ReLU)

    def derivative(self, x):
        '''
        :x: Entrada de la funcion sigmoide.
        :regresa: Derivada de la Relu
        '''
        x = x.detach().numpy()
        
        dv = np.clip(x>0, 0.0, 1.0)        
        return (Variable(torch.FloatTensor(dv),requires_grad=True))


class LeakyReLU_Layer(BaseLayer):

    def __init__(self,input_dim,output_dim):
        '''
        :param input_dim: input dimension of layer
        :param output_dim: output dimension of layer
        '''
        BaseLayer.__init__(self, input_dim, output_dim, funcion_activacion=Leaky_ReLU)


    def derivative(self, x):
        '''
        :x: Entrada de la funcion sigmoide.
        :regresa: Derivada de la Relu con valor minimo de 0.01.
        '''
        x = x.detach().numpy()
        
        dv = np.clip(x>0, 0.01, 1.00)        
        return (Variable(torch.FloatTensor(dv),requires_grad=True))
        

class Softmax_Layer(BaseLayer):

    def __init__(self,input_dim,output_dim):
        '''
        :param input_dim: input dimension of layer
        :param output_dim: output dimension of layer
        '''
        BaseLayer.__init__(self, input_dim, output_dim, funcion_activacion=softmax)


    def derivative(self, x):
        '''
        :param x: Softmax activated input.
        :return: derivate of softmax function.
        '''
        'pass'
        #print(x)
        x = x.detach().numpy() 
        lis = np.diag(x[0])
        dim = lis.shape
        lis = np.zeros(dim)            

        for i in range(len(x)):
            pred = x[i]
            dv = np.diag(x[i]) # Jacobiano
            for i in range(len(dv)):
                for j in range(len(dv)):
                    if i == j:
                        dv[i][j] = pred[i] * (1-pred[i])   # dv[i][j] = x[i] * (1-x[j])
                    else: 
                        dv[i][j] = -pred[i]*pred[j]
            for i in range(len(lis)):      # Para sumar todos las derivadas de cada muestra 
                for j in range(len(lis[0])):
                    lis[i][j] = lis[i][j]+dv[i][j]            
        
        lis = lis/(len(x))

        return (Variable(torch.FloatTensor(lis),requires_grad=True))


class SoftmaxBay_Layer(BaseLayer):

    def __init__(self,input_dim,output_dim):
        '''
        :param input_dim: input dimension of layer
        :param output_dim: output dimension of layer
        '''
        BaseLayer.__init__(self, input_dim, output_dim, funcion_activacion=softmax_Bay)


    def derivative(self, x):
        '''
        :param x: Softmax activated input.
        :return: derivate of softmax function.
        '''
        'pass'
        #print(x)
        x = x.detach().numpy() 
        lis = np.diag(x[0])
        dim = lis.shape
        lis = np.zeros(dim)            

        for i in range(len(x)):
            pred = x[i]
            dv = np.diag(x[i]) # Jacobiano
            for i in range(len(dv)):
                for j in range(len(dv)):
                    if i == j:
                        dv[i][j] = pred[i] * (1-pred[i])   # dv[i][j] = x[i] * (1-x[j])
                    else: 
                        dv[i][j] = -pred[i]*pred[j]
            for i in range(len(lis)):      # Para sumar todos las derivadas de cada muestra 
                for j in range(len(lis[0])):
                    lis[i][j] = lis[i][j]+dv[i][j]            
        
        lis = lis/(len(x))

        return (Variable(torch.FloatTensor(lis),requires_grad=True))


class Log_Softmax_Layer(BaseLayer):
    
    def __init__(self,input_dim,output_dim):
        '''
        :param input_dim: input dimension of layer
        :param output_dim: output dimension of layer
        '''
        BaseLayer.__init__(self, input_dim, output_dim, funcion_activacion=log_softmax)


    def derivative(self, x):
        '''
        :param x: Softmax activated input.
        :return: derivate of softmax function.
        '''
        'pass'
        #print(x)
        x = x.detach().numpy() 
        lis = np.diag(x[0])
        dim = lis.shape
        lis = np.zeros(dim)            

        for i in range(len(x)):
            pred = x[i]
            dv = np.diag(x[i]) # Jacobiano
            for i in range(len(dv)):
                for j in range(len(dv)):
                    if i == j:
                        dv[i][j] = pred[i] * (1-pred[i])   # dv[i][j] = x[i] * (1-x[j])
                    else: 
                        dv[i][j] = -pred[i]*pred[j]
            for i in range(len(lis)):      # Para sumar todos las derivadas de cada muestra 
                for j in range(len(lis[0])):
                    lis[i][j] = lis[i][j]+dv[i][j]            
        
        lis = lis/(len(x))

        return (Variable(torch.FloatTensor(lis),requires_grad=True))

